{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7ad48c40-c577-4352-af16-a5901e8e5c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "##This is a general file for the creation of all the StackExchange Golden Layer##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "939bff5e-d933-4b8e-af80-27cf33798935",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable\n",
    "os.environ[\"HADOOP_HOME\"] = \"C:/Spark/spark-3.5.5-bin-hadoop3\"  \n",
    "os.environ[\"HADOOP_OPTS\"] = \"-Djava.library.path=C:/Spark/spark-3.5.5-bin-hadoop3/bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "356f453e-abd1-497a-8640-91703fa5bf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import sys, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "951cbeae-229c-4a8c-a2da-650d4e6e9cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (col, date_format, when, sum,\n",
    "    count, lower, regexp_replace,\n",
    "    trim, lit, udf, year, month,\n",
    "    dayofmonth, weekofyear,\n",
    "    quarter, dayofweek, monotonically_increasing_id, row_number)\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, TimestampType\n",
    "from pyspark.sql.window import Window\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01dd6ec7-17a4-4a1f-a5d2-0d16cc049567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"Date Dimension Table\")\\\n",
    "    .config(\"spark.executor.memory\", \"3g\")\\\n",
    "    .config(\"spark.jars.packages\", \"com.databricks:spark-xml_2.12:0.16.0,org.apache.parquet:parquet-hadoop:1.15.1\")\\\n",
    "    .config(\"spark.pyspark.python\", sys.executable) \\\n",
    "    .config(\"spark.pyspark.driver.python\", sys.executable) \\\n",
    "    .config(\"spark.hadoop.io.native.lib.available\", \"false\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f81f360c-d0ab-4998-8682-adbb136bbdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Date Dim\n",
    "# Define Date Range\n",
    "start_date = \"2009-01-01\"\n",
    "end_date = \"2026-12-31\"\n",
    "dates_df = spark.sql(f\"SELECT sequence(to_date('{start_date}'), to_date('{end_date}'), interval 1 day) AS date_list\") \\\n",
    "    .selectExpr(\"explode(date_list) AS FullDate\")\n",
    "\n",
    "# Add Date Attributes\n",
    "Date_Dim = dates_df.withColumn(\"DateKey\", date_format(col(\"FullDate\"), \"yyyyMMdd\").cast(\"int\")) \\\n",
    "    .withColumn(\"DayOfMonth\", dayofmonth(col(\"FullDate\"))) \\\n",
    "    .withColumn(\"Month\", month(col(\"FullDate\"))) \\\n",
    "    .withColumn(\"Year\", year(col(\"FullDate\"))) \\\n",
    "    .withColumn(\"DayOfWeek\", dayofweek(col(\"FullDate\"))) \\\n",
    "    .withColumn(\"DayName\", date_format(col(\"FullDate\"), \"EEEE\")) \\\n",
    "    .withColumn(\"WeekOfYear\", weekofyear(col(\"FullDate\"))) \\\n",
    "    .withColumn(\"MonthName\", date_format(col(\"FullDate\"), \"MMMM\")) \\\n",
    "    .withColumn(\"Quarter\", quarter(col(\"FullDate\"))) \\\n",
    "    .withColumn(\"FiscalYear\", when(month(col(\"FullDate\")) >= 7, year(col(\"FullDate\")) + 1)\n",
    "                               .otherwise(year(col(\"FullDate\"))))\n",
    "# Save as Parquet (or change to CSV, Delta, etc.)\n",
    "Date_Dim.write.mode(\"overwrite\").parquet(\"Gold/Date_Dim\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3607176d-7fc4-4649-ab8b-bb553ad621ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformations On Tags File\n",
    "Tags = spark.read.parquet(r\"Silver/Tags\")\n",
    "\n",
    "Tags_window_spec = Window.orderBy(\"Id\")  \n",
    "\n",
    "Tags_Dim = Tags.withColumn(\"Tag_SK\", row_number().over(Tags_window_spec))\\\n",
    "            .withColumnRenamed(\"Id\", \"Tag_BK\")\\\n",
    "            .drop(\"TagDescPostId\")\\\n",
    "            .withColumnRenamed(\"Count\",\"Total_Count\")\n",
    "\n",
    "#Writing the file\n",
    "Tags_Dim_path = \"Gold/Tags_Dim\"\n",
    "Tags_Dim.write.mode(\"overwrite\").parquet(Tags_Dim_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5dfee04-5683-45b2-afa9-57e44add81ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformations On Users File\n",
    "Users = spark.read.parquet(r\"Silver/Users\")\n",
    "\n",
    "Users = Users.withColumnRenamed(\"UsersId_BK\",\"User_BK\")\n",
    "Users_window_spec = Window.orderBy(\"User_BK\")\n",
    "Users_Dim = Users.withColumn(\"User_SK\", row_number().over(Users_window_spec))\n",
    "\n",
    "#Adding the Null User\n",
    "new_row = spark.sql(\"SELECT -1 AS User_SK,NULL AS Views, -2 AS User_BK,\\\n",
    "                    NULL AS DisplayName, NULL AS Reputation,\\\n",
    "                    NULL AS CreationDate,NUll AS DownVotes, NULL AS LastAccessDate, NULL AS UpVotes\")\n",
    "Users_Dim = Users_Dim.unionByName(new_row)\n",
    "\n",
    "#Writing the file\n",
    "Users_Dim_path = \"Gold/Users_Dim\"\n",
    "Users_Dim.write.mode(\"overwrite\").parquet(Users_Dim_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed0bbb1d-3854-4c15-ade7-62f27d8b4148",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformations On Badges File\n",
    "Badges = spark.read.parquet(r\"Silver/Badges\")\n",
    "\n",
    "Badges = Badges.withColumnRenamed('Badge_Desc_Id','Badge_BK')\n",
    "\n",
    "Badges_window_spec = Window.orderBy(\"Badge_BK\")\n",
    "\n",
    "Badges_Dim = Badges.withColumn(\"Badge_SK\", row_number().over(Badges_window_spec))\n",
    "\n",
    "#Writing the file\n",
    "Badges_Dim_path = \"Gold/Badges_Dim\"\n",
    "Badges_Dim.write.mode(\"overwrite\").parquet(Badges_Dim_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e236425-9c5a-4bba-aa98-c4fadb60032c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformations On Questions File\n",
    "Questions = spark.read.parquet(r\"Silver\\Questions\")\n",
    "\n",
    "Questions_Dim= Questions.withColumnRenamed(\"QuestionId\",\"Question_BK\")\n",
    "\n",
    "Questions_window_spec = Window.orderBy(\"Question_BK\")\n",
    "\n",
    "Questions_Dim = Questions_Dim.drop(\"LastActivityDate\",\"Score\",\"ViewCount\",\"AnswerCount\",\"CommentCount\",\"Tags\",)\\\n",
    "                         .withColumn(\"Question_SK\", row_number().over(Questions_window_spec))\n",
    "\n",
    "#Adding the Null User\n",
    "new_row0 = spark.sql(\"SELECT -1 AS Question_SK,NULL AS Title, -1 AS Question_BK,\\\n",
    "                    NULL AS OwnerUserId, NULL AS Body, NULL AS CreationDate,NULL AS AcceptedAnswerId\")\n",
    "Questions_Dim = Questions_Dim.unionByName(new_row0)\n",
    "\n",
    "#Writing the file\n",
    "Questions_Dim_path = \"Gold/Questions_Dim\"\n",
    "Questions_Dim.write.mode(\"overwrite\").parquet(Questions_Dim_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cac9f6dc-8380-45c4-af78-2b211229fef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformations On Answers File\n",
    "Answers = spark.read.parquet(r\"Silver\\Answers\")\n",
    "\n",
    "Answers_Dim= Answers.withColumnRenamed(\"AnswerId\",\"Answer_BK\")\n",
    "\n",
    "Answers_window_spec = Window.orderBy(\"Answer_BK\")\n",
    "\n",
    "Answers_Dim = Answers_Dim.drop(\"CreationDate\",\"LastActivityDate\",\"Score\",\"CommentCount\")\\\n",
    "                         .withColumn(\"Answer_SK\", row_number().over(Answers_window_spec))\n",
    "#Adding the Null User\n",
    "new_row1 = spark.sql(\"SELECT -1 AS Answer_BK,NULL AS ParentId,\\\n",
    "                     NULL AS OwnerUserId, NULL AS Body, -1 AS Answer_SK\")\n",
    "Answers_Dim = Answers_Dim.unionByName(new_row1)\n",
    "\n",
    "\n",
    "#Writing the file\n",
    "Answers_Dim_path = \"Gold/Answers_Dim\"\n",
    "Answers_Dim.write.mode(\"overwrite\").parquet(Answers_Dim_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bda2ad-673c-43e2-af7f-d34432800af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Answer_BK|ParentId|OwnerUserId|                Body|Answer_SK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a01f51d-c722-4c5a-a5d6-d9a5443bed09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebf78b0-8002-46fa-878a-0ca3b2661421",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82110254-354b-4744-bf6f-7d5c9bf39f95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark Kernel",
   "language": "python",
   "name": "kernal_name"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
